{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "torch.backends.cudnn.benchmark=True\n",
    "import time\n",
    "\n",
    "from adamw import AdamW\n",
    "from scheduler import Scheduler\n",
    "from models import *\n",
    "\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('../../data')\n",
    "MODEL_PATH = PATH/'models'\n",
    "MODEL_PATH.mkdir(exist_ok=True)\n",
    "save_tag = 'imagenet_magenta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing\n",
    "def get_parser():\n",
    "    parser = argparse.ArgumentParser(description='PyTorch ImageNet Training')\n",
    "#     parser.add_argument('data', metavar='DIR', help='path to dataset')\n",
    "    parser.add_argument('--phases', type=str, help='Learning rate schedule')\n",
    "    parser.add_argument('-j', '--workers', default=8, type=int, help='number of data loading workers (default: 8)')\n",
    "    parser.add_argument('--start-epoch', default=0, type=int, help='manual epoch number (useful on restarts)')\n",
    "    parser.add_argument('--momentum', default=0.9, type=float, help='momentum')\n",
    "    parser.add_argument('--print-freq', '-p', default=5, type=int, help='print every')\n",
    "    parser.add_argument('--dist-url', default='env://', type=str, help='url used to set up distributed training')\n",
    "    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')\n",
    "    parser.add_argument('--local_rank', default=0, type=int, help='Used for multi-process training')\n",
    "    return parser\n",
    "\n",
    "# Distributed\n",
    "def reduce_tensor(tensor): return sum_tensor(tensor)/env_world_size()\n",
    "def sum_tensor(tensor):\n",
    "    rt = tensor.clone()\n",
    "    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n",
    "    return rt\n",
    "def env_world_size(): return int(os.environ.get('WORLD_SIZE', 1))\n",
    "def env_rank(): return int(os.environ.get('RANK',0))\n",
    "\n",
    "args = get_parser().parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_distributed = env_world_size() > 1\n",
    "if args.local_rank > 0:\n",
    "    f = open('/dev/null', 'w')\n",
    "    sys.stdout = f\n",
    "    \n",
    "if is_distributed:\n",
    "    print('Distributed initializing process group')\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url, world_size=env_world_size())\n",
    "    assert(env_world_size() == dist.get_world_size())\n",
    "    print(\"Distributed: success (%d/%d)\"%(args.local_rank, dist.get_world_size()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp16 = True\n",
    "mean,std = imagenet_stats\n",
    "if fp16: \n",
    "    mean = tensor(mean).half()\n",
    "    std = tensor(std).half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGENET_PATH = PATH/'imagenet-sz/160/train'\n",
    "# train_ds = ImageClassificationDataset.from_folder(IMAGENET_PATH)\n",
    "COCO_PATH = PATH/'coco/resize'\n",
    "train_ds = ImageClassificationDataset.from_folder(COCO_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size,bs = 96,36\n",
    "size,bs = 128,32\n",
    "# size,bs = 256,20\n",
    "\n",
    "# Content Data\n",
    "data_norm,data_denorm = normalize_funcs(mean, std)\n",
    "train_tds = DatasetTfm(train_ds, tfms=[crop_pad(size=size, is_random=False)], tfm_y=False, size=size, do_crop=True)\n",
    "\n",
    "data_sampler = DistributedSampler(train_tds, num_replicas=env_world_size(), rank=env_rank()) if is_distributed else None\n",
    "train_dl = DeviceDataLoader.create(train_tds, tfms=data_norm, num_workers=8, bs=bs, shuffle=(data_sampler is None), sampler=data_sampler)\n",
    "train_dl = DeviceDataLoader.create(train_tds, tfms=[to_half, data_norm], num_workers=8, bs=bs, shuffle=(data_sampler is None), sampler=data_sampler)\n",
    "\n",
    "# Style Data\n",
    "\n",
    "# STYLE_PATH = PATH/'style/dtd/images'\n",
    "STYLE_PATH = PATH/'style/dtd/subset'\n",
    "style_ds = ImageClassificationDataset.from_folder(STYLE_PATH)\n",
    "\n",
    "# STYLE_PATH = PATH/'style/pbn/train'\n",
    "# style_ds = ImageClassificationDataset.from_single_folder(STYLE_PATH, ['train'])\n",
    "\n",
    "style_tds = DatasetTfm(style_ds, tfms=[crop_pad(size=size, is_random=False)], tfm_y=False, size=size, do_crop=True)\n",
    "style_dl = DeviceDataLoader.create(style_tds, tfms=data_norm, num_workers=8, bs=1, shuffle=True)\n",
    "style_dl = DeviceDataLoader.create(style_tds, tfms=[to_half, data_norm], num_workers=8, bs=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if fp16:\n",
    "#     train_dl.add_tfm(to_half)\n",
    "#     style_dl.add_tfm(to_half)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Functions\n",
    "# losses\n",
    "def ct_loss(input, target): return F.mse_loss(input,target)\n",
    "\n",
    "def gram(input):\n",
    "        b,c,h,w = input.size()\n",
    "        x = input.view(b, c, -1)\n",
    "        return torch.bmm(x, x.transpose(1,2))/(c*h*w)\n",
    "\n",
    "def gram_loss(input, target): return F.mse_loss(gram(input), gram(target))\n",
    "\n",
    "def tva_loss(y):\n",
    "    w_var = torch.sum(torch.abs(y[:, :, :, :-1] - y[:, :, :, 1:]))\n",
    "    h_var = torch.sum(torch.abs(y[:, :, :-1, :] - y[:, :, 1:, :]))\n",
    "    return w_var + h_var\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "def reduce_tensor(tensor): return sum_tensor(tensor)/env_world_size()\n",
    "def sum_tensor(tensor):\n",
    "    rt = tensor.clone()\n",
    "    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n",
    "    return rt\n",
    "\n",
    "def env_world_size(): return int(os.environ.get('WORLD_SIZE', 1))\n",
    "def env_rank(): return int(os.environ.get('RANK',0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses\n",
    "def ct_loss(input, target): return F.mse_loss(input,target)\n",
    "\n",
    "def gram(input):\n",
    "        b,c,h,w = input.size()\n",
    "        x = input.view(b, c, -1)\n",
    "        return torch.bmm(x, x.transpose(1,2))/(c*h*w)\n",
    "\n",
    "def gram_loss(input, target): return F.mse_loss(gram(input), gram(target))\n",
    "\n",
    "def tva_loss(y):\n",
    "    w_var = torch.sum(torch.abs(y[:, :, :, :-1] - y[:, :, :, 1:]))\n",
    "    h_var = torch.sum(torch.abs(y[:, :, :-1, :] - y[:, :, 1:, :]))\n",
    "    return w_var + h_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models\n",
    "mt = StyleTransformer()\n",
    "ms = StylePredict.create_inception()\n",
    "m_com = CombinedModel(mt, ms).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer ids:  [12, 22, 32, 42]\n"
     ]
    }
   ],
   "source": [
    "m_vgg = VGGActivations().cuda()\n",
    "\n",
    "if is_distributed: \n",
    "    m_com = DistributedDataParallel(m_com, device_ids=[args.local_rank], output_device=args.local_rank)\n",
    "    m_vgg = DistributedDataParallel(m_vgg, device_ids=[args.local_rank], output_device=args.local_rank)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fp16_utils import FP16_Optimizer, network_to_half, BN_convert_float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "log_interval = 50\n",
    "optimizer = AdamW(m_com.parameters(), lr=1e-5, betas=(0.9,0.999), weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP16_Optimizer processing param group 0:\n",
      "FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([32, 3, 9, 9])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([32])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([32])\n",
      "FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([64, 32, 3, 3])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([64])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([64])\n",
      "FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([128, 64, 3, 3])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([128])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([128])\n",
      "FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([128, 128, 3, 3])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([128])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([128])\n",
      "FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([128, 128, 3, 3])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([128])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([128])\n",
      "FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([128, 128, 3, 3])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([128])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([128])\n",
      "FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([128, 128, 3, 3])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([128])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([128])\n",
      "FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([128, 128, 3, 3])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([128])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([128])\n",
      "FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([128, 128, 3, 3])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([128])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([128])\n",
      "FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([128, 128, 3, 3])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([128])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([128])\n",
      "FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([128, 128, 3, 3])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([128])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([128])\n",
      "FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([128, 128, 3, 3])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([128])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([128])\n",
      "FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([128, 128, 3, 3])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([128])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([128])\n",
      "FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([64, 128, 3, 3])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([64])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([64])\n",
      "FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([32, 64, 3, 3])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([32])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([32])\n",
      "FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([3, 32, 9, 9])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([3])\n",
      "FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([3])\n",
      "FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([100, 768, 1, 1])\n",
      "FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([100])\n",
      "FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([2758, 100, 1, 1])\n",
      "FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([2758])\n"
     ]
    }
   ],
   "source": [
    "if fp16:\n",
    "    m_com = BN_convert_float(m_com.half())\n",
    "    m_vgg = BN_convert_float(m_vgg.half())\n",
    "    optimizer = FP16_Optimizer(optimizer,\n",
    "                               static_loss_scale=256,\n",
    "                               dynamic_loss_scale=True);\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_mult = env_world_size()\n",
    "scheduler = Scheduler(optimizer, [{'ep': (0,1),      'lr': (1e-5*lr_mult,5e-4*lr_mult)}, \n",
    "                                  {'ep': (1,2),      'lr': (5e-4*lr_mult,1e-5*lr_mult)},\n",
    "                                  {'ep': (2,epochs), 'lr': (1e-5*lr_mult,1e-7*lr_mult)}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_wgts = [i*1e9 for i in [5,50,5,.5]] # 2,3,4,5\n",
    "c_block = 1 # 1=3\n",
    "ct_wgt = 1e3\n",
    "tva_wgt = 5e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m_com.load_state_dict(torch.load(MODEL_PATH/f'model_combined_4_imagenet_256.pth'), strict=False)\n",
    "# m_com.load_state_dict(torch.load(MODEL_PATH/f'model_combined_{save_tag}.pth'), strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_con,_ = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Half but got scalar type Float for argument #3 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-c2b1b0f7c43e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfp16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/arbitrary_style/training/fp16_utils/fp16_optimizer.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, update_master_grads)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0;31m# backward pass with retain_graph=False to tear down the graph.  Doing this would avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;31m# discarding the iteration,  but probably wouldn't improve overall efficiency.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mupdate_master_grads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_master_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/arbitrary_style/training/fp16_utils/loss_scaler.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mscaled_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;31m##############################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \"\"\"\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Half but got scalar type Float for argument #3 'weight'"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "m_com.train()\n",
    "style_image_count = 0\n",
    "for e in range(epochs):\n",
    "    agg_content_loss = 0.\n",
    "    agg_style_loss = 0.\n",
    "    agg_tva_loss = 0.\n",
    "    count = 0\n",
    "    batch_tot = len(train_dl)\n",
    "    for batch_id, (x_con,_) in enumerate(train_dl):\n",
    "        scheduler.update_lr(e, batch_id, batch_tot)\n",
    "        \n",
    "#         if (batch_id % (log_interval*3) == 0):\n",
    "        try:\n",
    "            x_style,_ = next(it_style)\n",
    "            style_image_count += 1\n",
    "        except:\n",
    "            it_style = iter(style_dl)\n",
    "            x_style,_ = next(it_style)\n",
    "            print('Restarting style')\n",
    "            style_image_count = 1\n",
    "            \n",
    "        with torch.no_grad(): \n",
    "            style_batch = x_style.repeat(bs,1,1,1)\n",
    "            s_out = m_vgg(style_batch)\n",
    "            style_feat = [s.clone() for s in s_out]\n",
    "            \n",
    "            targ_feat = m_vgg(x_con)[c_block].clone()\n",
    "            \n",
    "        n_batch = x_con.size(0)\n",
    "        count += n_batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = m_com(x_con, x_style)\n",
    "        out,_ = data_norm((out,None))\n",
    "        inp_feat = m_vgg(out)\n",
    "        \n",
    "        closs = [ct_loss(inp_feat[c_block],targ_feat) * ct_wgt]\n",
    "        sloss = [gram_loss(inp,targ)*wgt for inp,targ,wgt in zip(inp_feat, style_feat, style_wgts) if wgt > 0]\n",
    "        tvaloss = tva_loss(out) * tva_wgt\n",
    "        \n",
    "        total_loss = closs + sloss + [tvaloss]\n",
    "        total_loss = sum(total_loss)\n",
    "    \n",
    "        if fp16:\n",
    "            optimizer.backward(total_loss)\n",
    "        else:\n",
    "            total_loss.backward()\n",
    "            \n",
    "#         nn.utils.clip_grad_norm_(m_com.m_tran.parameters(), 10)\n",
    "#         nn.utils.clip_grad_norm_(m_com.m_style.parameters(), 80)\n",
    "        optimizer.step()\n",
    "    \n",
    "        mom = 0.9\n",
    "        agg_content_loss = agg_content_loss*mom + sum(closs).detach().data*(1-mom)\n",
    "        agg_style_loss = agg_style_loss*mom + sum(sloss).detach().data*(1-mom)\n",
    "        agg_tva_loss = agg_tva_loss*mom + tvaloss.detach().data*(1-mom)\n",
    "        agg_total_loss = (agg_content_loss + agg_style_loss + agg_tva_loss)\n",
    "        \n",
    "        if (batch_id + 1) % log_interval == 0:\n",
    "            time_elapsed = (time.time() - start)/60\n",
    "            b_tot = len(train_dl)\n",
    "            mesg = (f\"MIN:{time_elapsed:.2f}\\tEP[{e+1}]\\tB[{batch_id+1:4}/{b_tot}]\\t\"\n",
    "                    f\"CON:{agg_content_loss:.3f}\\tSTYL:{agg_style_loss:.2f}\\t\"\n",
    "                    f\"TVA:{agg_tva_loss:.2f}\\tTOT:{agg_total_loss:.2f}\\t\"\n",
    "#                     f\"S/CT:{style_image_count:3}/{count:3}\"\n",
    "                   )\n",
    "            print(mesg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/ubuntu/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m(90)\u001b[0;36mbackward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     88 \u001b[0;31m    Variable._execution_engine.run_backward(\n",
      "\u001b[0m\u001b[0;32m     89 \u001b[0;31m        \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 90 \u001b[0;31m        allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0m\u001b[0;32m     91 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     92 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> up\n",
      "> \u001b[0;32m/home/ubuntu/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/tensor.py\u001b[0m(96)\u001b[0;36mbackward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     94 \u001b[0;31m                \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     95 \u001b[0;31m        \"\"\"\n",
      "\u001b[0m\u001b[0;32m---> 96 \u001b[0;31m        \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     97 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     98 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> up\n",
      "> \u001b[0;32m/home/ubuntu/arbitrary_style/training/fp16_utils/loss_scaler.py\u001b[0m(132)\u001b[0;36mbackward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    130 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    131 \u001b[0;31m        \u001b[0mscaled_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 132 \u001b[0;31m        \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    133 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    134 \u001b[0;31m\u001b[0;31m##############################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> scaled_loss.type()\n",
      "'torch.cuda.FloatTensor'\n",
      "ipdb> scaled_loss.half().backward()\n",
      "*** RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.\n",
      "ipdb> quit\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restarting style\n",
      "Changing LR from 1e-05 to 1.001223867922172e-05\n",
      "MIN:0.18\tEP[1]\tB[  50/40037]\tCON:21.521\tSTYL:360.43\tTVA:0.27\tTOT:382.22\t\n",
      "MIN:0.30\tEP[1]\tB[ 100/40037]\tCON:21.607\tSTYL:129.86\tTVA:0.40\tTOT:151.86\t\n",
      "MIN:0.42\tEP[1]\tB[ 150/40037]\tCON:21.065\tSTYL:166.64\tTVA:0.53\tTOT:188.23\t\n",
      "MIN:0.53\tEP[1]\tB[ 200/40037]\tCON:21.459\tSTYL:295.78\tTVA:0.67\tTOT:317.90\t\n",
      "MIN:0.65\tEP[1]\tB[ 250/40037]\tCON:21.166\tSTYL:165.36\tTVA:0.80\tTOT:187.33\t\n",
      "MIN:0.76\tEP[1]\tB[ 300/40037]\tCON:21.221\tSTYL:161.49\tTVA:0.91\tTOT:183.63\t\n",
      "MIN:0.88\tEP[1]\tB[ 350/40037]\tCON:21.484\tSTYL:84.43\tTVA:0.99\tTOT:106.90\t\n",
      "MIN:0.99\tEP[1]\tB[ 400/40037]\tCON:21.730\tSTYL:283.69\tTVA:1.05\tTOT:306.47\t\n",
      "MIN:1.11\tEP[1]\tB[ 450/40037]\tCON:21.404\tSTYL:200.80\tTVA:1.25\tTOT:223.45\t\n",
      "Restarting style\n",
      "MIN:1.23\tEP[1]\tB[ 500/40037]\tCON:21.450\tSTYL:206.04\tTVA:1.37\tTOT:228.87\t\n",
      "MIN:1.35\tEP[1]\tB[ 550/40037]\tCON:21.330\tSTYL:179.09\tTVA:1.56\tTOT:201.97\t\n",
      "MIN:1.46\tEP[1]\tB[ 600/40037]\tCON:21.096\tSTYL:120.15\tTVA:1.59\tTOT:142.83\t\n",
      "MIN:1.58\tEP[1]\tB[ 650/40037]\tCON:21.560\tSTYL:187.18\tTVA:1.61\tTOT:210.35\t\n",
      "MIN:1.70\tEP[1]\tB[ 700/40037]\tCON:21.754\tSTYL:156.63\tTVA:1.72\tTOT:180.10\t\n",
      "MIN:1.81\tEP[1]\tB[ 750/40037]\tCON:21.705\tSTYL:198.29\tTVA:1.84\tTOT:221.84\t\n",
      "MIN:1.93\tEP[1]\tB[ 800/40037]\tCON:21.768\tSTYL:80.43\tTVA:1.93\tTOT:104.13\t\n",
      "MIN:2.04\tEP[1]\tB[ 850/40037]\tCON:21.760\tSTYL:106.38\tTVA:1.97\tTOT:130.11\t\n",
      "MIN:2.16\tEP[1]\tB[ 900/40037]\tCON:22.111\tSTYL:120.14\tTVA:1.96\tTOT:144.21\t\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-c2b1b0f7c43e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m#         nn.utils.clip_grad_norm_(m_com.m_tran.parameters(), 10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m#         nn.utils.clip_grad_norm_(m_com.m_style.parameters(), 80)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mmom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                     \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "m_com.train()\n",
    "style_image_count = 0\n",
    "for e in range(epochs):\n",
    "    agg_content_loss = 0.\n",
    "    agg_style_loss = 0.\n",
    "    agg_tva_loss = 0.\n",
    "    count = 0\n",
    "    batch_tot = len(train_dl)\n",
    "    for batch_id, (x_con,_) in enumerate(train_dl):\n",
    "        scheduler.update_lr(e, batch_id, batch_tot)\n",
    "        \n",
    "#         if (batch_id % (log_interval*3) == 0):\n",
    "        try:\n",
    "            x_style,_ = next(it_style)\n",
    "            style_image_count += 1\n",
    "        except:\n",
    "            it_style = iter(style_dl)\n",
    "            x_style,_ = next(it_style)\n",
    "            print('Restarting style')\n",
    "            style_image_count = 1\n",
    "            \n",
    "        with torch.no_grad(): \n",
    "            style_batch = x_style.repeat(bs,1,1,1)\n",
    "            s_out = m_vgg(style_batch)\n",
    "            style_feat = [s.clone() for s in s_out]\n",
    "            \n",
    "            targ_feat = m_vgg(x_con)[c_block].clone()\n",
    "            \n",
    "        n_batch = x_con.size(0)\n",
    "        count += n_batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = m_com(x_con, x_style)\n",
    "        out,_ = data_norm((out,None))\n",
    "        inp_feat = m_vgg(out)\n",
    "        \n",
    "        closs = [ct_loss(inp_feat[c_block],targ_feat) * ct_wgt]\n",
    "        sloss = [gram_loss(inp,targ)*wgt for inp,targ,wgt in zip(inp_feat, style_feat, style_wgts) if wgt > 0]\n",
    "        tvaloss = tva_loss(out) * tva_wgt\n",
    "        \n",
    "        total_loss = closs + sloss + [tvaloss]\n",
    "        total_loss = sum(total_loss)\n",
    "    \n",
    "        if fp16:\n",
    "            optimizer.backward(total_loss)\n",
    "        else:\n",
    "            total_loss.backward()\n",
    "            \n",
    "#         nn.utils.clip_grad_norm_(m_com.m_tran.parameters(), 10)\n",
    "#         nn.utils.clip_grad_norm_(m_com.m_style.parameters(), 80)\n",
    "        optimizer.step()\n",
    "    \n",
    "        mom = 0.9\n",
    "        agg_content_loss = agg_content_loss*mom + sum(closs).detach().data*(1-mom)\n",
    "        agg_style_loss = agg_style_loss*mom + sum(sloss).detach().data*(1-mom)\n",
    "        agg_tva_loss = agg_tva_loss*mom + tvaloss.detach().data*(1-mom)\n",
    "        agg_total_loss = (agg_content_loss + agg_style_loss + agg_tva_loss)\n",
    "        \n",
    "        if (batch_id + 1) % log_interval == 0:\n",
    "            time_elapsed = (time.time() - start)/60\n",
    "            b_tot = len(train_dl)\n",
    "            mesg = (f\"MIN:{time_elapsed:.2f}\\tEP[{e+1}]\\tB[{batch_id+1:4}/{b_tot}]\\t\"\n",
    "                    f\"CON:{agg_content_loss:.3f}\\tSTYL:{agg_style_loss:.2f}\\t\"\n",
    "                    f\"TVA:{agg_tva_loss:.2f}\\tTOT:{agg_total_loss:.2f}\\t\"\n",
    "#                     f\"S/CT:{style_image_count:3}/{count:3}\"\n",
    "                   )\n",
    "            print(mesg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restarting style\n",
      "Changing LR from 1e-05 to 1.001223867922172e-05\n",
      "MIN:0.23\tEP[1]\tB[  50/40037]\tCON:21.390\tSTYL:208.79\tTVA:0.55\tTOT:230.73\t\n",
      "MIN:0.35\tEP[1]\tB[ 100/40037]\tCON:21.632\tSTYL:221.01\tTVA:0.75\tTOT:243.39\t\n",
      "MIN:0.48\tEP[1]\tB[ 150/40037]\tCON:21.328\tSTYL:141.67\tTVA:0.93\tTOT:163.93\t\n",
      "MIN:0.61\tEP[1]\tB[ 200/40037]\tCON:21.494\tSTYL:113.39\tTVA:1.16\tTOT:136.04\t\n",
      "MIN:0.73\tEP[1]\tB[ 250/40037]\tCON:21.622\tSTYL:87.02\tTVA:1.36\tTOT:110.01\t\n",
      "MIN:0.86\tEP[1]\tB[ 300/40037]\tCON:21.507\tSTYL:151.13\tTVA:1.49\tTOT:174.12\t\n",
      "MIN:0.99\tEP[1]\tB[ 350/40037]\tCON:21.483\tSTYL:135.05\tTVA:1.68\tTOT:158.21\t\n",
      "MIN:1.12\tEP[1]\tB[ 400/40037]\tCON:21.700\tSTYL:134.94\tTVA:1.77\tTOT:158.40\t\n",
      "MIN:1.24\tEP[1]\tB[ 450/40037]\tCON:21.594\tSTYL:81.95\tTVA:1.91\tTOT:105.45\t\n",
      "Restarting style\n",
      "MIN:1.38\tEP[1]\tB[ 500/40037]\tCON:21.775\tSTYL:121.79\tTVA:1.98\tTOT:145.54\t\n",
      "MIN:1.50\tEP[1]\tB[ 550/40037]\tCON:21.886\tSTYL:155.54\tTVA:1.94\tTOT:179.36\t\n",
      "MIN:1.63\tEP[1]\tB[ 600/40037]\tCON:22.104\tSTYL:398.23\tTVA:2.19\tTOT:422.52\t\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-c2b1b0f7c43e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mstyle_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms_out\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mtarg_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm_vgg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_con\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc_block\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mn_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_con\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/arbitrary_style/training/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstored\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 313\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "m_com.train()\n",
    "style_image_count = 0\n",
    "for e in range(epochs):\n",
    "    agg_content_loss = 0.\n",
    "    agg_style_loss = 0.\n",
    "    agg_tva_loss = 0.\n",
    "    count = 0\n",
    "    batch_tot = len(train_dl)\n",
    "    for batch_id, (x_con,_) in enumerate(train_dl):\n",
    "        scheduler.update_lr(e, batch_id, batch_tot)\n",
    "        \n",
    "#         if (batch_id % (log_interval*3) == 0):\n",
    "        try:\n",
    "            x_style,_ = next(it_style)\n",
    "            style_image_count += 1\n",
    "        except:\n",
    "            it_style = iter(style_dl)\n",
    "            x_style,_ = next(it_style)\n",
    "            print('Restarting style')\n",
    "            style_image_count = 1\n",
    "            \n",
    "        with torch.no_grad(): \n",
    "            style_batch = x_style.repeat(bs,1,1,1)\n",
    "            s_out = m_vgg(style_batch)\n",
    "            style_feat = [s.clone() for s in s_out]\n",
    "            \n",
    "            targ_feat = m_vgg(x_con)[c_block].clone()\n",
    "            \n",
    "        n_batch = x_con.size(0)\n",
    "        count += n_batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = m_com(x_con, x_style)\n",
    "        out,_ = data_norm((out,None))\n",
    "        inp_feat = m_vgg(out)\n",
    "        \n",
    "        closs = [ct_loss(inp_feat[c_block],targ_feat) * ct_wgt]\n",
    "        sloss = [gram_loss(inp,targ)*wgt for inp,targ,wgt in zip(inp_feat, style_feat, style_wgts) if wgt > 0]\n",
    "        tvaloss = tva_loss(out) * tva_wgt\n",
    "        \n",
    "        total_loss = closs + sloss + [tvaloss]\n",
    "        total_loss = sum(total_loss)\n",
    "    \n",
    "        if fp16:\n",
    "            optimizer.backward(total_loss)\n",
    "        else:\n",
    "            total_loss.backward()\n",
    "            \n",
    "#         nn.utils.clip_grad_norm_(m_com.m_tran.parameters(), 10)\n",
    "#         nn.utils.clip_grad_norm_(m_com.m_style.parameters(), 80)\n",
    "        optimizer.step()\n",
    "    \n",
    "        mom = 0.9\n",
    "        agg_content_loss = agg_content_loss*mom + sum(closs).detach().data*(1-mom)\n",
    "        agg_style_loss = agg_style_loss*mom + sum(sloss).detach().data*(1-mom)\n",
    "        agg_tva_loss = agg_tva_loss*mom + tvaloss.detach().data*(1-mom)\n",
    "        agg_total_loss = (agg_content_loss + agg_style_loss + agg_tva_loss)\n",
    "        \n",
    "        if (batch_id + 1) % log_interval == 0:\n",
    "            time_elapsed = (time.time() - start)/60\n",
    "            b_tot = len(train_dl)\n",
    "            mesg = (f\"MIN:{time_elapsed:.2f}\\tEP[{e+1}]\\tB[{batch_id+1:4}/{b_tot}]\\t\"\n",
    "                    f\"CON:{agg_content_loss:.3f}\\tSTYL:{agg_style_loss:.2f}\\t\"\n",
    "                    f\"TVA:{agg_tva_loss:.2f}\\tTOT:{agg_total_loss:.2f}\\t\"\n",
    "#                     f\"S/CT:{style_image_count:3}/{count:3}\"\n",
    "                   )\n",
    "            print(mesg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_imgs(x_con, x_style, idx=0):\n",
    "    with torch.no_grad(): \n",
    "        out = m_com(x_con, x_style)\n",
    "    fig, axs = plt.subplots(1,3,figsize=(12,4))\n",
    "    Image(data_denorm(x_con[idx].cpu())).show(axs[0])\n",
    "    axs[0].set_title('Content')\n",
    "    axs[1].set_title('Style')\n",
    "    axs[2].set_title('Transfer')\n",
    "    Image(data_denorm(x_style[0].cpu())).show(axs[1])\n",
    "    Image((out[idx].detach().cpu())).show(axs[2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_con,_ = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_style,_ = next(iter(style_dl))\n",
    "eval_imgs(x_con, x_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_style,_ = next(iter(style_dl))\n",
    "eval_imgs(x_con, x_style)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
