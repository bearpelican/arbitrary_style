{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "torch.backends.cudnn.benchmark=True\n",
    "import time\n",
    "\n",
    "from adamw import AdamW\n",
    "from scheduler import Scheduler, LRScheduler\n",
    "from models import *\n",
    "from loss import TransferLoss\n",
    "from data import ContentStyleLoader, InputDataset, SimpleDataBunch\n",
    "from dist import DDP, sum_tensor, reduce_tensor, env_world_size, env_rank\n",
    "\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PATH = Path('../../data')\n",
    "if not PATH.exists():\n",
    "    PATH = Path('../data')\n",
    "# PATH = Path('/ncluster/models/resnet_test.pth')\n",
    "# MODEL_PATH = Path('/ncluster/models')\n",
    "# MODEL_PATH.mkdir(exist_ok=True)\n",
    "# save_tag = 'resnet_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing\n",
    "def get_parser():\n",
    "    parser = argparse.ArgumentParser(description='PyTorch ImageNet Training')\n",
    "#     parser.add_argument('data', metavar='DIR', help='path to dataset')\n",
    "    parser.add_argument('--phases', type=str, help='Learning rate schedule')\n",
    "    parser.add_argument('-j', '--workers', default=8, type=int, help='number of data loading workers (default: 8)')\n",
    "    parser.add_argument('--start-epoch', default=0, type=int, help='manual epoch number (useful on restarts)')\n",
    "    parser.add_argument('--momentum', default=0.9, type=float, help='momentum')\n",
    "    parser.add_argument('--print-freq', '-p', default=5, type=int, help='print every')\n",
    "    parser.add_argument('--dist-url', default='env://', type=str, help='url used to set up distributed training')\n",
    "    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')\n",
    "    parser.add_argument('--local_rank', default=0, type=int, help='Used for multi-process training')\n",
    "    return parser\n",
    "\n",
    "args = get_parser().parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_distributed = env_world_size() > 1\n",
    "if args.local_rank > 0:\n",
    "    f = open('/dev/null', 'w')\n",
    "    sys.stdout = f\n",
    "    \n",
    "if is_distributed:\n",
    "    print('Distributed initializing process group')\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url, world_size=env_world_size())\n",
    "    assert(env_world_size() == dist.get_world_size())\n",
    "    print(\"Distributed: success (%d/%d)\"%(args.local_rank, dist.get_world_size()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size,bs = 96,36\n",
    "# size,bs = 128,32\n",
    "size,bs = 256,2\n",
    "\n",
    "data_norm,data_denorm = normalize_funcs(*imagenet_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGENET_PATH = PATH/'imagenet-sz/320/train'\n",
    "# imagenet_files = get_files(IMAGENET_PATH, recurse=True)\n",
    "\n",
    "COCO_PATH = PATH/'coco/resize'\n",
    "coco_files = get_files(COCO_PATH, recurse=True)\n",
    "cont_ds = InputDataset(coco_files)\n",
    "\n",
    "# Content Data\n",
    "cont_tds = DatasetTfm(cont_ds, tfms=[crop_pad(size=size, is_random=False), flip_lr(p=0.5)], tfm_y=False, size=size, do_crop=True)\n",
    "data_sampler = DistributedSampler(cont_tds, num_replicas=env_world_size(), rank=env_rank()) if is_distributed else None\n",
    "cont_dl = DeviceDataLoader.create(cont_tds, tfms=data_norm, num_workers=8, \n",
    "                                   bs=bs, shuffle=(data_sampler is None), sampler=data_sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Style Data\n",
    "\n",
    "STYLE_PATH_DTD = PATH/'style/dtd/images'\n",
    "dtd_files = get_files(STYLE_PATH_DTD, recurse=True)\n",
    "\n",
    "# STYLE_PATH_PBN = PATH/'style/pbn/train'\n",
    "# pbn_files = get_files(STYLE_PATH_PBN, recurse=True)\n",
    "\n",
    "style_ds = InputDataset(dtd_files)\n",
    "style_tds = DatasetTfm(style_ds, tfms=[crop_pad(size=size, is_random=False), flip_lr(p=0.5)], tfm_y=False, size=size, do_crop=True)\n",
    "style_dl = DeviceDataLoader.create(style_tds, tfms=data_norm, num_workers=8, bs=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = ContentStyleLoader(cont_dl, style_dl, repeat_xy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models\n",
    "mt = StyleTransformer()\n",
    "ms = StylePredict.create_resnet()\n",
    "# ms = StylePredict.create_inception()\n",
    "m_com = CombinedModel(mt, ms).cuda()\n",
    "if is_distributed: \n",
    "    m_com = DDP(m_com, device_ids=[args.local_rank], output_device=args.local_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "log_interval = 50\n",
    "optimizer = AdamW(m_com.parameters(), lr=1e-5, betas=(0.9,0.999), weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_mult = env_world_size()\n",
    "scheduler = LRScheduler(optimizer, [{'ep': (0,1),      'lr': (1e-5*lr_mult,5e-4*lr_mult)}, \n",
    "                                  {'ep': (1,2),      'lr': (5e-4*lr_mult,1e-5*lr_mult)},\n",
    "                                  {'ep': (2,epochs), 'lr': (1e-5*lr_mult,1e-7*lr_mult)}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_wgt = 2.5e9\n",
    "ct_wgt = 5e2\n",
    "tva_wgt = 1e-6\n",
    "st_block_wgts = [1,80,200,5] # 2,3,4,5\n",
    "c_block = 1 # 1=3\n",
    "lr_mult = env_world_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer ids:  [12, 22, 32, 42]\n"
     ]
    }
   ],
   "source": [
    "m_vgg = VGGActivations().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_loss = TransferLoss(m_vgg, ct_wgt, st_wgt, st_block_wgts, tva_wgt, data_norm, c_block, sum_loss=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m_com.load_state_dict(torch.load(MODEL_PATH/f'model_combined_4_imagenet_256.pth'), strict=False)\n",
    "# m_com.load_state_dict(torch.load(MODEL_PATH/f'{save_tag}.pth'), strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing LR from 1e-05 to 1.0008285004142502e-05\n",
      "MIN:0.16\tEP[1]\tB[  50/59143]\tCON:10.052\tSTYL:103.82\tTVA:0.07\tTOT:113.94\tS/CT:  0/100\n",
      "MIN:0.22\tEP[1]\tB[ 100/59143]\tCON:10.165\tSTYL:117.69\tTVA:0.07\tTOT:127.92\tS/CT:  0/200\n",
      "MIN:0.28\tEP[1]\tB[ 150/59143]\tCON:9.723\tSTYL:86.59\tTVA:0.06\tTOT:96.38\tS/CT:  0/300\n",
      "MIN:0.34\tEP[1]\tB[ 200/59143]\tCON:10.130\tSTYL:87.29\tTVA:0.06\tTOT:97.48\tS/CT:  0/400\n",
      "MIN:0.40\tEP[1]\tB[ 250/59143]\tCON:9.596\tSTYL:93.95\tTVA:0.06\tTOT:103.61\tS/CT:  0/500\n",
      "MIN:0.46\tEP[1]\tB[ 300/59143]\tCON:9.983\tSTYL:69.37\tTVA:0.07\tTOT:79.42\tS/CT:  0/600\n",
      "MIN:0.53\tEP[1]\tB[ 350/59143]\tCON:9.978\tSTYL:70.44\tTVA:0.07\tTOT:80.48\tS/CT:  0/700\n",
      "MIN:0.59\tEP[1]\tB[ 400/59143]\tCON:10.310\tSTYL:96.92\tTVA:0.07\tTOT:107.30\tS/CT:  0/800\n",
      "MIN:0.65\tEP[1]\tB[ 450/59143]\tCON:10.513\tSTYL:74.34\tTVA:0.08\tTOT:84.93\tS/CT:  0/900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/fastai/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/ubuntu/anaconda3/envs/fastai/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/ubuntu/anaconda3/envs/fastai/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/ubuntu/anaconda3/envs/fastai/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-49b19ddb0e2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm_com\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_con\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_style\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mcloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtvaloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_con\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_style\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai/fastai/vision/data.py\u001b[0m in \u001b[0;36m_normalize_batch\u001b[0;34m(b, mean, std, do_y)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;34m\"`b` = `x`,`y` - normalize `x` array of imgs and `do_y` optionally `y`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m     \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdo_y\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "m_com.train()\n",
    "style_image_count = 0\n",
    "for e in range(epochs):\n",
    "    agg_content_loss = 0.\n",
    "    agg_style_loss = 0.\n",
    "    agg_tva_loss = 0.\n",
    "    count = 0\n",
    "    batch_tot = len(train_dl)\n",
    "    for batch_id, (x_con,x_style) in enumerate(train_dl):\n",
    "        scheduler.update_lr(e, batch_id, batch_tot)\n",
    "            \n",
    "        n_batch = x_con.size(0)\n",
    "        count += n_batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = m_com(x_con, x_style)\n",
    "        out,_ = data_norm((out,None))\n",
    "        \n",
    "        closs, sloss, tvaloss = m_loss(out, x_con, x_style)\n",
    "        \n",
    "        total_loss = closs + sloss + [tvaloss]\n",
    "        total_loss = sum(total_loss)\n",
    "    \n",
    "        total_loss.backward()\n",
    "        m_clip = m_com.module if is_distributed else m_com\n",
    "        nn.utils.clip_grad_norm_(m_clip.m_tran.parameters(), 10)\n",
    "        nn.utils.clip_grad_norm_(m_clip.m_style.parameters(), 100)\n",
    "        optimizer.step()\n",
    "    \n",
    "        mom = 0.9\n",
    "        agg_content_loss = agg_content_loss*mom + sum(closs).detach().data*(1-mom)\n",
    "        agg_style_loss = agg_style_loss*mom + sum(sloss).detach().data*(1-mom)\n",
    "        agg_tva_loss = agg_tva_loss*mom + tvaloss.detach().data*(1-mom)\n",
    "        agg_total_loss = (agg_content_loss + agg_style_loss + agg_tva_loss)\n",
    "\n",
    "        if is_distributed: # Must keep track of global batch size, since not all machines are guaranteed equal batches at the end of an epoch\n",
    "            metrics = torch.tensor([agg_content_loss, agg_style_loss, agg_tva_loss, agg_total_loss]).float().cuda()\n",
    "            agg_content_loss, agg_style_loss, agg_tva_loss, agg_total_loss = reduce_tensor(metrics).cpu().numpy()\n",
    "        \n",
    "        if (batch_id + 1) % log_interval == 0:\n",
    "            time_elapsed = (time.time() - start)/60\n",
    "            mesg = (f\"MIN:{time_elapsed:.2f}\\tEP[{e+1}]\\tB[{batch_id+1:4}/{batch_tot}]\\t\"\n",
    "                    f\"CON:{agg_content_loss:.3f}\\tSTYL:{agg_style_loss:.2f}\\t\"\n",
    "                    f\"TVA:{agg_tva_loss:.2f}\\tTOT:{agg_total_loss:.2f}\\t\"\n",
    "                    f\"S/CT:{style_image_count:3}/{count:3}\"\n",
    "                   )\n",
    "            print(mesg)\n",
    "\n",
    "        save_interval = 1000\n",
    "        if (args.local_rank == 0) and (batch_id+1) % save_interval == 0:\n",
    "            if args.save:\n",
    "                print('Saving model: ', args.save)\n",
    "                save_path = Path(args.save).expanduser()\n",
    "                save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                torch.save(m_com.state_dict(), save_path)\n",
    "\n",
    "                ep_save = save_path.with_name(f'{save_path.stem}_{e}').with_suffix(save_path.suffix)\n",
    "\n",
    "                print('Saving epoch checkpoint: ', ep_save)\n",
    "                torch.save(m_com.state_dict(), ep_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_imgs(x_con, x_style, model=m_com, idx=0):\n",
    "    with torch.no_grad(): \n",
    "        out = m_com(x_con, x_style)\n",
    "    fig, axs = plt.subplots(1,3,figsize=(12,4))\n",
    "    Image(data_denorm(x_con[idx].cpu())).show(axs[0])\n",
    "    axs[0].set_title('Content')\n",
    "    axs[1].set_title('Style')\n",
    "    axs[2].set_title('Transfer')\n",
    "    Image(data_denorm(x_style[0].cpu())).show(axs[1])\n",
    "    Image((out[idx].detach().cpu())).show(axs[2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_con,_ = next(iter(train_dl))\n",
    "idx=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_style,_ = next(iter(style_dl))\n",
    "eval_imgs(x_con, x_style, idx=idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_style,_ = next(iter(style_dl))\n",
    "eval_imgs(x_con, x_style, idx=idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
